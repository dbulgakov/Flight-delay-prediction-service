{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.files.file_helper import load_binary_file, save_binary_file\n",
    "import utils.configuration\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.configuration.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_DATA_LOCATION = config.config['DEFAULT']['MERGED_DATA_LOCATION']\n",
    "MERGED_DATA_FILE_BIN = config.config['DEFAULT']['MERGED_DATA_FILE_BIN']\n",
    "PREPROCESSED_DATA_FILE_BIN = config.config['DEFAULT']['PREPROCESSED_DATA_FILE_BIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependent_variable_value(cancelled, delay_duration):\n",
    "    if cancelled == 1:\n",
    "        return 'cancelled_flight'\n",
    "    if delay_duration > 60:\n",
    "        return 'long_delay'\n",
    "    if delay_duration > 15:\n",
    "        return 'delay'\n",
    "    return 'no_delay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airline_delay_index(delay_df, carrier):\n",
    "    return delay_df[delay_df['carrier'] == carrier]['delay_index'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weather(data_df, prefix):\n",
    "    data_df[prefix + 'awnd'].fillna((data_df[prefix + 'awnd'].mean()), inplace=True)\n",
    "    \n",
    "    data_df.loc[:, data_df.columns.str.startswith(prefix)] = data_df \\\n",
    "                                                .loc[:, data_df.columns.str.startswith(prefix)].fillna(value=0)\n",
    "    \n",
    "    \n",
    "    data_df[prefix + 'fog'] = list(map(int , (data_df[prefix + 'wt01'] + data_df[prefix + 'wt02']).values > 0))\n",
    "    data_df[prefix + 'hail'] = list(map(int , (data_df[prefix + 'wt04'] + data_df[prefix + 'wt05']).values > 0))\n",
    "    data_df[prefix + 'damaging_wind'] = list(map(int , (data_df[prefix + 'wt10'] + data_df[prefix + 'wt11']).values > 0))\n",
    "    data_df[prefix + 'snow'] = list(map(int , (data_df[prefix + 'snow'] + data_df[prefix + 'wt09']).values > 0))\n",
    "    \n",
    "    data_df = data_df.rename(columns={prefix + 'snwd': prefix + 'snow_depth', \n",
    "                                      prefix + 'awnd': prefix + 'average_wind_speed', \n",
    "                                      prefix + 'wt03': prefix + 'thunder', \n",
    "                                      prefix + 'wt07': prefix + 'dust', \n",
    "                                      prefix + 'wt08': prefix + 'haze'})\n",
    "    \n",
    "    data_df[data_df.columns.drop(list(data_df.filter(regex=prefix + 'wt')))]\n",
    "    \n",
    "    return data_df.loc[:, ~data_df.columns.str.startswith(prefix + 'wt')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_airline_rating(data_df):\n",
    "    airlines_group = data_df[['status', 'op_unique_carrier']]\n",
    "    airlines_group_num = airlines_group.groupby(['op_unique_carrier']).size()\n",
    "    airlines_group = data_df[['status', 'op_unique_carrier']]\n",
    "    airlines_group = airlines_group[(airlines_group['status'] != 'no_delay')]\n",
    "    airlines_group_delays_num = airlines_group.groupby(['op_unique_carrier']).size()\n",
    "    delay_info = pd.DataFrame({'op_unique_carrier': np.unique(airlines_group.op_unique_carrier.values), 'number_of_flights': airlines_group_num.values, 'number_of_delays': airlines_group_delays_num.values})\n",
    "    delay_info['delay_index'] = delay_info['number_of_delays'] / delay_info['number_of_flights']\n",
    "    \n",
    "    return delay_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_variables(file_data):\n",
    "    \n",
    "    data_df = file_data.copy()\n",
    "    \n",
    "    data_df.columns = map(str.lower, data_df.columns)\n",
    "    \n",
    "    logging.debug('Processing origin weather')\n",
    "    \n",
    "    data_df = process_weather(data_df, 'origin_')\n",
    "    \n",
    "    logging.debug('Processing dest weather')\n",
    "\n",
    "    data_df = process_weather(data_df, 'dest_')\n",
    "        \n",
    "    logging.debug('Creating dependent variable')\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(16) as pool:\n",
    "        data_df['status'] = list(pool.map(get_dependent_variable_value, data_df['cancelled'], data_df['arr_delay'], chunksize=1_000))\n",
    "    \n",
    "    logging.debug('Processing airline ratings')\n",
    "    \n",
    "    ratings = process_airline_rating(data_df)\n",
    "    \n",
    "    data_df = pd.merge(data_df, ratings[['op_unique_carrier', 'delay_index']], on=['op_unique_carrier'])\n",
    "    \n",
    "    logging.debug('Peforming final data preparation')\n",
    "    \n",
    "    data_df['crs_dep_time'] = list(map(int, data_df['crs_dep_time'].values / 100))    \n",
    "    \n",
    "    data_df['day_of_year'] = (data_df['fl_date'] - data_df['fl_date'].min())  / np.timedelta64(1,'D')\n",
    "    \n",
    "    data_df['weekend'] = np.where(data_df['day_of_week'] >= 6, 1, 0)\n",
    "    \n",
    "    data_df = data_df.drop(['dep_delay', 'arr_delay', 'cancelled', 'crs_elapsed_time', \n",
    "                           'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay',\n",
    "                           'late_aircraft_delay'], axis=1)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(merged_data_location, merged_data_file_name, preprocessed_file_name):\n",
    "    logging.info('Loading historical data')\n",
    "\n",
    "    parsed_data = load_binary_file(merged_data_location, merged_data_file_name)\n",
    "    \n",
    "    logging.info('Starting to process data file')\n",
    "    \n",
    "    processed_data = process_data_variables(parsed_data)\n",
    "    \n",
    "    logging.info('Saving preprocessed data file')\n",
    "    \n",
    "    save_binary_file(processed_data, preprocessed_file_name, merged_data_location)\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading historical data\n",
      "INFO:root:Starting to process data file\n",
      "INFO:root:Saving preprocessed data file\n"
     ]
    }
   ],
   "source": [
    "parsed_data = process_data(MERGED_DATA_LOCATION, MERGED_DATA_FILE_BIN, PREPROCESSED_DATA_FILE_BIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
