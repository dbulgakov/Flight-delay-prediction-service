{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.progress.log_progress import log_progress\n",
    "from utils.files.file_helper import ensure_directory, get_all_files_from_subfolders\n",
    "import utils.configuration\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.configuration.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORICAL_DATA_FOLDER = config.config['DEFAULT']['FLIGHT_DATA']\n",
    "WEATHER_DATA_FOLDER = config.config['DEFAULT']['WEATHER_DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_historical_data(historical_data_folder):\n",
    "    logging.info('Loading historical data')\n",
    "    all_data_files = get_all_files_from_subfolders(historical_data_folder)\n",
    "    \n",
    "    all_data_array = []\n",
    "    \n",
    "    str_array = ['OP_UNIQUE_CARRIER', 'TAIL_NUM', 'ORIGIN', 'ORIGIN_CITY_NAME', 'DEST', 'DEST_CITY_NAME', 'CRS_ELAPSED_TIME']\n",
    "    \n",
    "    for data_file in log_progress(all_data_files):\n",
    "        data = pd.read_csv(data_file, \n",
    "                           dtype={'QUARTER': int, 'FL_NUM': str,\n",
    "                                 'OP_UNIQUE_CARRIER': str, 'TAIL_NUM': str,\n",
    "                                 'ORIGIN_AIRPORT_ID': int, 'ORIGIN_AIRPORT_SEQ_ID': int,\n",
    "                                 'ORIGIN_AIRPORT_SEQ_ID': int, 'ORIGIN_CITY_MARKET_ID': int,\n",
    "                                 'ORIGIN': str, 'ORIGIN_CITY_NAME': str, \n",
    "                                 'DEST_AIRPORT_ID': int, 'DEST_AIRPORT_SEQ_ID': int,\n",
    "                                 'DEST_CITY_MARKET_ID': int, 'DEST': str,\n",
    "                                 'DEST_CITY_NAME': str, 'CRS_DEP_TIME': int,\n",
    "                                 'DEP_DELAY': float, 'ARR_DELAY': float,\n",
    "                                 'CANCELLED': int, 'CANCELLATION_CODE': str,\n",
    "                                 'DIVERTED': int, 'CRS_ELAPSED_TIME': str,\n",
    "                                 'CARRIER_DELAY': float, 'WEATHER_DELAY': float,\n",
    "                                 'NAS_DELAY': float, 'SECURITY_DELAY': float,\n",
    "                                 'LATE_AIRCRAFT_DELAY': float\n",
    "                                 }, \n",
    "                           parse_dates=['FL_DATE'])\n",
    "        \n",
    "        \n",
    "        data[str_array] = data[str_array].astype(str)\n",
    "        \n",
    "        all_data_array.append(data)\n",
    "    \n",
    "    logging.info('Concating loaded historical data chunks')\n",
    "    all_data = pd.concat(all_data_array)\n",
    "\n",
    "    return pd.concat(all_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_historical_data(original_data):\n",
    "    logging.info('Preprocessing historical data')\n",
    "    \n",
    "    original_data = original_data.drop(['ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', \n",
    "                                        'ORIGIN_CITY_MARKET_ID', 'DEST_AIRPORT_ID',\n",
    "                                        'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID',\n",
    "                                        'CANCELLATION_CODE', 'Unnamed: 27'], axis=1)\n",
    "    original_data.update(original_data[['DEP_DELAY','ARR_DELAY','CARRIER_DELAY', \n",
    "                                       'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY',\n",
    "                                       'LATE_AIRCRAFT_DELAY']].fillna(0))\n",
    "    return original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_data(weather_data_folder):\n",
    "    logging.info('Loading weather data')\n",
    "    all_data_files = get_all_files_from_subfolders(weather_data_folder)\n",
    "    \n",
    "    all_data_array = []\n",
    "        \n",
    "    for data_file in log_progress(all_data_files):\n",
    "        data = pd.read_csv(data_file, \n",
    "                           dtype={'attributes': str, 'datatype': str, 'station': str,\n",
    "                                  'value': int}, \n",
    "                           parse_dates=['date'])\n",
    "        \n",
    "        data = data.pivot_table(index=['date'], columns='datatype', values='value').reset_index()\n",
    "        \n",
    "        data['city'] = data_file[data_file.rfind(\"/\")+1:][:-4]\n",
    "        \n",
    "        all_data_array.append(data)\n",
    "    \n",
    "    logging.info('Concating loaded weather data chunks')\n",
    "    all_data = pd.concat(all_data_array)\n",
    "\n",
    "    return pd.concat(all_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(historical_data, weather_data):\n",
    "    logging.info('Starting to merge data')\n",
    "    logging.info('Preparing origin data')\n",
    "    origin_weather = weather_data.copy()\n",
    "    origin_weather = origin_weather.rename(columns={'date': 'FL_DATE', 'city': 'ORIGIN_CITY_NAME'})\n",
    "    origin_weather[origin_weather.columns.difference(['ORIGIN_CITY_NAME', 'FL_DATE'])].add_prefix('origin_')\n",
    "    \n",
    "    logging.info('Preparing dest data')\n",
    "    dest_weather = weather_data.copy()\n",
    "    dest_weather = dest_weather.rename(columns={'date': 'FL_DATE', 'city': 'DEST_CITY_NAME'})\n",
    "    dest_weather[dest_weather.columns.difference(['DEST_CITY_NAME', 'FL_DATE'])].add_prefix('dest_')\n",
    "    \n",
    "    merged_data = historical_data.copy()\n",
    "    \n",
    "    merged_data.ORIGIN_CITY_NAME = merged_data.ORIGIN_CITY_NAME.astype(str)\n",
    "    merged_data.DEST_CITY_NAME = merged_data.DEST_CITY_NAME.astype(str)\n",
    "    origin_weather.ORIGIN_CITY_NAME = origin_weather.ORIGIN_CITY_NAME.astype(str)\n",
    "    dest_weather.DEST_CITY_NAME = dest_weather.DEST_CITY_NAME.astype(str)\n",
    "    \n",
    "    logging.info('Merging with origin data')\n",
    "    merged_data = pd.merge(merged_data, origin_weather, on=['ORIGIN_CITY_NAME', 'FL_DATE'])\n",
    "    \n",
    "    print(len(merged_data))\n",
    "    \n",
    "    logging.info('Merging with dest data')\n",
    "    merged_data = pd.merge(merged_data, dest_weather, on=['DEST_CITY_NAME', 'FL_DATE'])\n",
    "    \n",
    "    return merged_data\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading historical data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10edf5a920214b94a6cebff4fbdf7c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=12)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Concating loaded historical data chunks\n",
      "INFO:root:Preprocessing historical data\n"
     ]
    }
   ],
   "source": [
    "historical_data = get_all_historical_data(HISTORICAL_DATA_FOLDER)\n",
    "historical_data = preprocess_historical_data(historical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = get_all_weather_data(WEATHER_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_data(historical_data, weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(historical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
