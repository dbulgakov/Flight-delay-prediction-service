{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.progress.log_progress import log_progress\n",
    "from utils.files.file_helper import ensure_directory, get_all_files_from_subfolders, save_binary_file\n",
    "import utils.configuration\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.configuration.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORICAL_DATA_FOLDER = config.config['DEFAULT']['FLIGHT_DATA']\n",
    "WEATHER_DATA_FOLDER = config.config['DEFAULT']['WEATHER_DATA']\n",
    "MERGED_DATA_LOCATION = config.config['DEFAULT']['MERGED_DATA_LOCATION']\n",
    "MERGED_DATA_FILE = config.config['DEFAULT']['MERGED_DATA_FILE']\n",
    "MERGED_DATA_FILE_BIN = config.config['DEFAULT']['MERGED_DATA_FILE_BIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_historical_data(historical_data_folder):\n",
    "    logging.info('Loading historical data')\n",
    "    all_data_files = get_all_files_from_subfolders(historical_data_folder)\n",
    "    \n",
    "    all_data_array = []\n",
    "    \n",
    "    str_array = ['OP_UNIQUE_CARRIER', 'TAIL_NUM', 'ORIGIN', 'ORIGIN_CITY_NAME', 'DEST', 'DEST_CITY_NAME', 'CRS_ELAPSED_TIME']\n",
    "    \n",
    "    for data_file in log_progress(all_data_files):\n",
    "        data = pd.read_csv(data_file, \n",
    "                           dtype={'QUARTER': int, 'FL_NUM': str,\n",
    "                                 'OP_UNIQUE_CARRIER': str, 'TAIL_NUM': str,\n",
    "                                 'ORIGIN_AIRPORT_ID': int, 'ORIGIN_AIRPORT_SEQ_ID': int,\n",
    "                                 'ORIGIN_AIRPORT_SEQ_ID': int, 'ORIGIN_CITY_MARKET_ID': int,\n",
    "                                 'ORIGIN': str, 'ORIGIN_CITY_NAME': str, \n",
    "                                 'DEST_AIRPORT_ID': int, 'DEST_AIRPORT_SEQ_ID': int,\n",
    "                                 'DEST_CITY_MARKET_ID': int, 'DEST': str,\n",
    "                                 'DEST_CITY_NAME': str, 'CRS_DEP_TIME': int,\n",
    "                                 'DEP_DELAY': float, 'ARR_DELAY': float,\n",
    "                                 'CANCELLED': int, 'CANCELLATION_CODE': str,\n",
    "                                 'DIVERTED': int, 'CRS_ELAPSED_TIME': str,\n",
    "                                 'CARRIER_DELAY': float, 'WEATHER_DELAY': float,\n",
    "                                 'NAS_DELAY': float, 'SECURITY_DELAY': float,\n",
    "                                 'LATE_AIRCRAFT_DELAY': float\n",
    "                                 }, \n",
    "                           parse_dates=['FL_DATE'])\n",
    "        \n",
    "        \n",
    "        data[str_array] = data[str_array].astype(str)\n",
    "        \n",
    "        all_data_array.append(data)\n",
    "    \n",
    "    logging.info('Concating loaded historical data chunks')\n",
    "    all_data = pd.concat(all_data_array)\n",
    "\n",
    "    return pd.concat(all_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_historical_data(original_data):\n",
    "    logging.info('Preprocessing historical data')\n",
    "    \n",
    "    original_data = original_data.drop(['ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', \n",
    "                                        'ORIGIN_CITY_MARKET_ID', 'DEST_AIRPORT_ID',\n",
    "                                        'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID',\n",
    "                                        'CANCELLATION_CODE', 'Unnamed: 27'], axis=1)\n",
    "    original_data.update(original_data[['DEP_DELAY','ARR_DELAY','CARRIER_DELAY', \n",
    "                                       'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY',\n",
    "                                       'LATE_AIRCRAFT_DELAY']].fillna(0))\n",
    "    return original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_data(weather_data_folder):\n",
    "    logging.info('Loading weather data')\n",
    "    all_data_files = get_all_files_from_subfolders(weather_data_folder)\n",
    "    \n",
    "    all_data_array = []\n",
    "        \n",
    "    for data_file in log_progress(all_data_files):\n",
    "        data = pd.read_csv(data_file, \n",
    "                           dtype={'attributes': str, 'datatype': str, 'station': str,\n",
    "                                  'value': int}, \n",
    "                           parse_dates=['date'])\n",
    "        \n",
    "        data = data.pivot_table(index=['date'], columns='datatype', values='value').reset_index()\n",
    "        \n",
    "        data['city'] = data_file[data_file.rfind(\"/\")+1:][:-4]\n",
    "        \n",
    "        all_data_array.append(data)\n",
    "    \n",
    "    logging.info('Concating loaded weather data chunks')\n",
    "    all_data = pd.concat(all_data_array)\n",
    "\n",
    "    return pd.concat(all_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(weather_data):\n",
    "    selected_cols = ['city', 'date', 'AWND', 'SNOW', 'SNWD', 'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT07', 'WT08', 'WT09', 'WT10', 'WT11']\n",
    "    \n",
    "    return weather_data[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(historical_data, weather_data):\n",
    "    logging.info('Preparing origin data')\n",
    "    origin_weather = weather_data.copy()\n",
    "    origin_weather = origin_weather.rename(columns={'date': 'FL_DATE', 'city': 'ORIGIN_CITY_NAME'})\n",
    "    origin_cols = origin_weather.columns[~origin_weather.columns.str.contains('ORIGIN_CITY_NAME|FL_DATE')]\n",
    "    origin_weather.rename(columns = dict(zip(origin_cols, 'origin_' + origin_cols)), inplace=True)\n",
    "    \n",
    "    \n",
    "    logging.info('Preparing dest data')\n",
    "    dest_weather = weather_data.copy()\n",
    "    dest_weather = dest_weather.rename(columns={'date': 'FL_DATE', 'city': 'DEST_CITY_NAME'})\n",
    "    dest_weather[dest_weather.columns.difference(['DEST_CITY_NAME', 'FL_DATE'])] = dest_weather[dest_weather.columns.difference(['DEST_CITY_NAME', 'FL_DATE'])].add_prefix('dest_')\n",
    "    dest_cols = dest_weather.columns[~dest_weather.columns.str.contains('DEST_CITY_NAME|FL_DATE')]\n",
    "    dest_weather.rename(columns = dict(zip(dest_cols, 'dest_' + dest_cols)), inplace=True)\n",
    "   \n",
    "    \n",
    "    \n",
    "    merged_data = historical_data.copy()\n",
    "    \n",
    "    merged_data.ORIGIN_CITY_NAME = merged_data.ORIGIN_CITY_NAME.astype(str)\n",
    "    merged_data.DEST_CITY_NAME = merged_data.DEST_CITY_NAME.astype(str)\n",
    "    origin_weather.ORIGIN_CITY_NAME = origin_weather.ORIGIN_CITY_NAME.astype(str)\n",
    "    dest_weather.DEST_CITY_NAME = dest_weather.DEST_CITY_NAME.astype(str)\n",
    "    \n",
    "    logging.info('Merging with origin data')\n",
    "    merged_data = pd.merge(merged_data, origin_weather, on=['ORIGIN_CITY_NAME', 'FL_DATE'], how='left')\n",
    "    \n",
    "    logging.info('Merging with dest data')\n",
    "    merged_data = pd.merge(merged_data, dest_weather, on=['DEST_CITY_NAME', 'FL_DATE'], how='left')\n",
    "    \n",
    "    return merged_data\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_merge_process(historical_data_folder, weather_data_folder, merged_data_location, merged_data_csv_name, merged_data_bin_name):\n",
    "    logging.info('Starting to merge data')\n",
    "    historical_data = get_all_historical_data(historical_data_folder)\n",
    "    historical_data = preprocess_historical_data(historical_data)\n",
    "    \n",
    "    weather_data = get_all_weather_data(weather_data_folder)\n",
    "    weather_data = preprocess_weather_data(weather_data)\n",
    "    \n",
    "    merged_data = merge_data(historical_data, weather_data)\n",
    "    \n",
    "    logging.info('Total number of docs {}'.format(len(historical_data)))\n",
    "    logging.info('Merged number of docs {}'.format(len(merged_data)))\n",
    "    \n",
    "    logging.info('Saving merged data to csv')\n",
    "    merged_data.to_csv('{}/{}'.format(merged_data_location, merged_data_csv_name) \n",
    "                       ,chunksize=100000\n",
    "                       ,compression='gzip'\n",
    "                       ,index=False)\n",
    "    \n",
    "    logging.info('Saving merged data to binary file')\n",
    "    save_binary_file(merged_data, merged_data_bin_name, merged_data_location)\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting to merge data\n",
      "INFO:root:Loading historical data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b0d03b72f244acbcd4b042121b3749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=24)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Concating loaded historical data chunks\n",
      "INFO:root:Preprocessing historical data\n",
      "INFO:root:Loading weather data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6687a740074b15be763bd77d829e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=423)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Concating loaded weather data chunks\n",
      "INFO:root:Preparing origin data\n",
      "INFO:root:Preparing dest data\n",
      "INFO:root:Merging with origin data\n",
      "INFO:root:Merging with dest data\n",
      "INFO:root:Total number of docs 13531490\n",
      "INFO:root:Merged number of docs 13548434\n",
      "INFO:root:Saving merged data to csv\n",
      "INFO:root:Saving merged data to binary file\n"
     ]
    }
   ],
   "source": [
    "zz = perform_merge_process(HISTORICAL_DATA_FOLDER, WEATHER_DATA_FOLDER, MERGED_DATA_LOCATION, MERGED_DATA_FILE, MERGED_DATA_FILE_BIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
